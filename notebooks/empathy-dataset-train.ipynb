{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:27:57.290926Z",
     "iopub.status.busy": "2025-05-03T11:27:57.290691Z",
     "iopub.status.idle": "2025-05-03T11:27:57.314207Z",
     "shell.execute_reply": "2025-05-03T11:27:57.313579Z",
     "shell.execute_reply.started": "2025-05-03T11:27:57.290903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:50:19.283111Z",
     "iopub.status.busy": "2025-05-03T11:50:19.282814Z",
     "iopub.status.idle": "2025-05-03T11:50:20.205670Z",
     "shell.execute_reply": "2025-05-03T11:50:20.204983Z",
     "shell.execute_reply.started": "2025-05-03T11:50:19.283090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %rm -rf /kaggle/working/empathy_dataset_transfer\n",
    "# %cd /kaggle/working\n",
    "# !git clone https://bedanar:<GITHUB_TOKEN>@github.com/psytechlab/empathy_dataset_transfer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:50:21.566889Z",
     "iopub.status.busy": "2025-05-03T11:50:21.566600Z",
     "iopub.status.idle": "2025-05-03T11:50:21.720665Z",
     "shell.execute_reply": "2025-05-03T11:50:21.719710Z",
     "shell.execute_reply.started": "2025-05-03T11:50:21.566865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %cd /kaggle/working/empathy_dataset_transfer\n",
    "# !git checkout empathy-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:50:23.314375Z",
     "iopub.status.busy": "2025-05-03T11:50:23.313564Z",
     "iopub.status.idle": "2025-05-03T11:50:23.318439Z",
     "shell.execute_reply": "2025-05-03T11:50:23.317692Z",
     "shell.execute_reply.started": "2025-05-03T11:50:23.314346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"/kaggle/working/empathy_dataset_transfer\") \n",
    "# sys.path.append(\"/kaggle/working/empathy_dataset_transfer/src\") \n",
    "# sys.path.append(\"/kaggle/working/empathy_dataset_transfer/src/contrib\") \n",
    "# sys.path.append(\"/kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-03T11:50:23.635842Z",
     "iopub.status.busy": "2025-05-03T11:50:23.635090Z",
     "iopub.status.idle": "2025-05-03T11:50:23.642786Z",
     "shell.execute_reply": "2025-05-03T11:50:23.642093Z",
     "shell.execute_reply.started": "2025-05-03T11:50:23.635813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaConfig\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.contrib.empathy_models.models.models import BiEncoderAttentionWithRationaleClassification\n",
    "from src.contrib.empathy_models.evaluation_utils import *\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:44:20.560505Z",
     "iopub.status.busy": "2025-05-03T11:44:20.559951Z",
     "iopub.status.idle": "2025-05-03T11:44:20.564816Z",
     "shell.execute_reply": "2025-05-03T11:44:20.564112Z",
     "shell.execute_reply.started": "2025-05-03T11:44:20.560480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "lr = 2e-5\n",
    "lambda_EI = 1\n",
    "lambda_RE = 0.5\n",
    "dropout = 0.1\n",
    "max_len = 128\n",
    "batch_size = 4\n",
    "epochs = 5\n",
    "seed_val = 12\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:43:07.361945Z",
     "iopub.status.busy": "2025-05-03T11:43:07.361345Z",
     "iopub.status.idle": "2025-05-03T11:43:07.369164Z",
     "shell.execute_reply": "2025-05-03T11:43:07.368675Z",
     "shell.execute_reply.started": "2025-05-03T11:43:07.361925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:50:28.992047Z",
     "iopub.status.busy": "2025-05-03T11:50:28.991794Z",
     "iopub.status.idle": "2025-05-03T11:50:59.528696Z",
     "shell.execute_reply": "2025-05-03T11:50:59.527908Z",
     "shell.execute_reply.started": "2025-05-03T11:50:28.992030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/nafiska/smarty_intents/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Volumes/nafiska/smarty_intents/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Volumes/nafiska/smarty_intents/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# !python /kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models/process_data.py --input_path=/kaggle/input/health-empathy-dataset/emotional-reactions-reddit.csv --output_path=/kaggle/working/emotional-reactions-reddit.csv\n",
    "# !python /kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models/process_data.py --input_path=/kaggle/input/health-empathy-dataset/explorations-reddit.csv --output_path=/kaggle/working/explorations-reddit.csv\n",
    "# !python /kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models/process_data.py --input_path=/kaggle/input/health-empathy-dataset/interpretations-reddit.csv --output_path=/kaggle/working/interpretations-reddit.csv\n",
    "\n",
    "!python src/contrib/empathy_models/process_data.py --input_path=emotional-reactions-reddit.csv --output_path=cor_emotional-reactions-reddit.csv\n",
    "!python src/contrib/empathy_models/process_data.py --input_path=explorations-reddit.csv --output_path=cor_explorations-reddit.csv\n",
    "!python src/contrib/empathy_models/process_data.py --input_path=interpretations-reddit.csv --output_path=cor_interpretations-reddit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:51:10.243419Z",
     "iopub.status.busy": "2025-05-03T11:51:10.242758Z",
     "iopub.status.idle": "2025-05-03T11:51:10.343309Z",
     "shell.execute_reply": "2025-05-03T11:51:10.342495Z",
     "shell.execute_reply.started": "2025-05-03T11:51:10.243393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('emotional-reactions-reddit.csv', delimiter=',')\n",
    "# df_all = pd.read_csv('explorations-reddit.csv', delimiter=',')\n",
    "# df_all = pd.read_csv('interpretations-reddit.csv', delimiter=',')\n",
    "\n",
    "df, validation = train_test_split(df_all, train_size=0.7, random_state=42)\n",
    "df_val, df_test = train_test_split(validation, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:51:10.752080Z",
     "iopub.status.busy": "2025-05-03T11:51:10.751848Z",
     "iopub.status.idle": "2025-05-03T11:51:10.755481Z",
     "shell.execute_reply": "2025-05-03T11:51:10.754848Z",
     "shell.execute_reply.started": "2025-05-03T11:51:10.752062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_path = \"/kaggle/working/sample_input_ER_out.csv\"\n",
    "# dev_path = \"/kaggle/working/sample_input_ER_out.csv\"\n",
    "# test_path = \"/kaggle/working/sample_input_ER_out.csv\"\n",
    "save_model_path = '/kaggle/working/interpretations_model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:51:12.477938Z",
     "iopub.status.busy": "2025-05-03T11:51:12.477241Z",
     "iopub.status.idle": "2025-05-03T11:51:12.584952Z",
     "shell.execute_reply": "2025-05-03T11:51:12.584409Z",
     "shell.execute_reply.started": "2025-05-03T11:51:12.477914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(train_path, delimiter=',')\n",
    "df['rationale_labels'] = df['rationale_labels'].apply(lambda s: torch.tensor(np.asarray([int(i) for i in s.split(',')]), dtype=torch.long))\n",
    "\n",
    "# df_test = pd.read_csv(test_path, delimiter=',')\n",
    "df_test['rationale_labels'] = df_test['rationale_labels'].apply(lambda s: torch.tensor(np.asarray([int(i) for i in s.split(',')]), dtype=torch.long))\n",
    "\n",
    "# df_val = pd.read_csv(dev_path, delimiter=',')\n",
    "df_val['rationale_labels'] = df_val['rationale_labels'].apply(lambda s: torch.tensor(np.asarray([int(i) for i in s.split(',')]), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:51:12.934219Z",
     "iopub.status.busy": "2025-05-03T11:51:12.933625Z",
     "iopub.status.idle": "2025-05-03T11:51:13.623072Z",
     "shell.execute_reply": "2025-05-03T11:51:13.622154Z",
     "shell.execute_reply.started": "2025-05-03T11:51:12.934201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_RP = tokenizer.batch_encode_plus(df.response_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length', return_attention_mask=True)\n",
    "input_ids_RP = torch.tensor(tokenizer_RP['input_ids'])\n",
    "attention_masks_RP = torch.tensor(tokenizer_RP['attention_mask'])\n",
    "\n",
    "tokenizer_SP = tokenizer.batch_encode_plus(df.seeker_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length', return_attention_mask=True)\n",
    "input_ids_SP = torch.tensor(tokenizer_SP['input_ids'])\n",
    "attention_masks_SP = torch.tensor(tokenizer_SP['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:43:51.461651Z",
     "iopub.status.busy": "2025-05-03T11:43:51.461354Z",
     "iopub.status.idle": "2025-05-03T11:43:51.476368Z",
     "shell.execute_reply": "2025-05-03T11:43:51.475844Z",
     "shell.execute_reply.started": "2025-05-03T11:43:51.461630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = df.level.values.astype(int)\n",
    "labels = torch.tensor(labels)\n",
    "rationales = df.rationale_labels.values.tolist()\n",
    "rationales = torch.stack(rationales, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-03T11:43:52.381552Z",
     "iopub.status.busy": "2025-05-03T11:43:52.380943Z",
     "iopub.status.idle": "2025-05-03T11:43:52.481812Z",
     "shell.execute_reply": "2025-05-03T11:43:52.481274Z",
     "shell.execute_reply.started": "2025-05-03T11:43:52.381533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_tokenizer_RP = tokenizer.batch_encode_plus(df_val.response_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length',return_attention_mask=True)\n",
    "val_input_ids_RP = torch.tensor(val_tokenizer_RP['input_ids'])\n",
    "val_attention_masks_RP = torch.tensor(val_tokenizer_RP['attention_mask'])\n",
    "\n",
    "val_tokenizer_SP = tokenizer.batch_encode_plus(df_val.seeker_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length',return_attention_mask=True)\n",
    "val_input_ids_SP = torch.tensor(val_tokenizer_SP['input_ids'])\n",
    "val_attention_masks_SP = torch.tensor(val_tokenizer_SP['attention_mask'])\n",
    "\n",
    "val_labels = torch.tensor(df_val.level.values.astype(int))\n",
    "val_rationales = df_val.rationale_labels.values.tolist()\n",
    "val_rationales = torch.stack(val_rationales, dim=0)\n",
    "val_rationales_trimmed = torch.tensor(df_val.rationale_labels_trimmed.values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.333204Z",
     "iopub.status.idle": "2025-05-03T11:28:42.333405Z",
     "shell.execute_reply": "2025-05-03T11:28:42.333316Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.333307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_tokenizer_RP = tokenizer.batch_encode_plus(df_test.response_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length', return_attention_mask=True)\n",
    "test_input_ids_RP = torch.tensor(test_tokenizer_RP['input_ids'])\n",
    "test_attention_masks_RP = torch.tensor(test_tokenizer_RP['attention_mask'])\n",
    "\n",
    "test_tokenizer_SP = tokenizer.batch_encode_plus(df_test.seeker_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length', return_attention_mask=True)\n",
    "test_input_ids_SP = torch.tensor(test_tokenizer_SP['input_ids'])\n",
    "test_attention_masks_SP = torch.tensor(test_tokenizer_SP['attention_mask'])\n",
    "\n",
    "test_labels = torch.tensor(df_test.level.values.astype(int))\n",
    "test_rationales = df_test.rationale_labels.values.tolist()\n",
    "test_rationales = torch.stack(test_rationales, dim=0)\n",
    "test_rationales_trimmed = torch.tensor(df_test.rationale_labels_trimmed.values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.334534Z",
     "iopub.status.idle": "2025-05-03T11:28:42.334836Z",
     "shell.execute_reply": "2025-05-03T11:28:42.334710Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.334697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BiEncoderAttentionWithRationaleClassification(model_name=model_name, hidden_dropout_prob=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.335957Z",
     "iopub.status.idle": "2025-05-03T11:28:42.336179Z",
     "shell.execute_reply": "2025-05-03T11:28:42.336091Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.336081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "for p in model.seeker_encoder.parameters():\n",
    "\tp.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.337154Z",
     "iopub.status.idle": "2025-05-03T11:28:42.337456Z",
     "shell.execute_reply": "2025-05-03T11:28:42.337309Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.337297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = lr, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.338578Z",
     "iopub.status.idle": "2025-05-03T11:28:42.338878Z",
     "shell.execute_reply": "2025-05-03T11:28:42.338735Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.338722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_SP, attention_masks_SP, input_ids_RP, attention_masks_RP, labels, rationales)\n",
    "train_size = int(len(train_dataset))\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids_SP, val_attention_masks_SP, val_input_ids_RP, val_attention_masks_RP, val_labels, val_rationales, val_rationales_trimmed)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids_SP, test_attention_masks_SP, test_input_ids_RP, test_attention_masks_RP, test_labels, test_rationales, test_rationales_trimmed)\n",
    "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.340088Z",
     "iopub.status.idle": "2025-05-03T11:28:42.340345Z",
     "shell.execute_reply": "2025-05-03T11:28:42.340247Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.340235Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "num_batch = len(train_dataloader)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.341173Z",
     "iopub.status.idle": "2025-05-03T11:28:42.341376Z",
     "shell.execute_reply": "2025-05-03T11:28:42.341288Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.341279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_empathy_loss = 0\n",
    "    total_train_rationale_loss = 0\n",
    "    \n",
    "    pbar = tqdm(total=num_batch, desc=f\"training\")\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids_SP = batch[0].to(device)\n",
    "        b_input_mask_SP = batch[1].to(device)\n",
    "        b_input_ids_RP = batch[2].to(device)\n",
    "        b_input_mask_RP = batch[3].to(device)\n",
    "        b_labels = batch[4].to(device)\n",
    "        b_rationales = batch[5].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "        \n",
    "        loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = model(input_ids_SP = b_input_ids_SP,\n",
    "                                                                input_ids_RP = b_input_ids_RP, \n",
    "                                                                attention_mask_SP=b_input_mask_SP,\n",
    "                                                                attention_mask_RP=b_input_mask_RP, \n",
    "                                                                empathy_labels=b_labels,\n",
    "                                                                rationale_labels=b_rationales,\n",
    "                                                                lambda_EI=lambda_EI,\n",
    "                                                                lambda_RE=lambda_RE)\n",
    "        \n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        total_train_empathy_loss += loss_empathy.item()\n",
    "        total_train_rationale_loss += loss_rationale.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix_str(\n",
    "            f\"total loss: {float(total_train_loss/(step+1)):.4f} epoch: {epoch_i}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_empathy_loss = total_train_empathy_loss / len(train_dataloader)\n",
    "    avg_train_rationale_loss = total_train_rationale_loss / len(train_dataloader)\n",
    "    \n",
    "    pbar.close()\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy_empathy = 0\n",
    "    total_eval_accuracy_rationale = 0\n",
    "    \n",
    "    total_pos_f1_empathy = 0.0\n",
    "    total_micro_f1_empathy = 0.0\n",
    "    total_macro_f1_empathy = 0.0\n",
    "    \n",
    "    total_pos_f1_rationale = 0.0\n",
    "    total_micro_f1_rationale = 0.0\n",
    "    total_macro_f1_rationale = 0.0\n",
    "    \n",
    "    total_iou_rationale = 0.0\n",
    "    \n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:   \n",
    "        b_input_ids_SP = batch[0].to(device)\n",
    "        b_input_mask_SP = batch[1].to(device)\n",
    "        b_input_ids_RP = batch[2].to(device)\n",
    "        b_input_mask_RP = batch[3].to(device)\n",
    "        b_labels = batch[4].to(device)\n",
    "        b_rationales = batch[5].to(device)\t\n",
    "        b_rationales_trimmed = batch[6].to(device)\t\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = model(input_ids_SP = b_input_ids_SP,\n",
    "                                                                input_ids_RP = b_input_ids_RP, \n",
    "                                                                attention_mask_SP=b_input_mask_SP,\n",
    "                                                                attention_mask_RP=b_input_mask_RP, \n",
    "                                                                empathy_labels=b_labels,\n",
    "                                                                rationale_labels=b_rationales,\n",
    "                                                                lambda_EI=lambda_EI,\n",
    "                                                                lambda_RE=lambda_RE)\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        logits_empathy = logits_empathy.detach().cpu().numpy()\n",
    "        logits_rationale = logits_rationale.detach().cpu().numpy()\n",
    "        \n",
    "        label_empathy_ids = b_labels.to('cpu').numpy()\n",
    "        label_rationale_ids = b_rationales.to('cpu').numpy()\n",
    "        rationale_lens = b_rationales_trimmed.to('cpu').numpy()\n",
    "        \n",
    "        total_eval_accuracy_empathy += flat_accuracy(logits_empathy, label_empathy_ids, axis_=1)\n",
    "        total_eval_accuracy_rationale += flat_accuracy_rationale(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "        \n",
    "        pos_f1_empathy, micro_f1_empathy, macro_f1_empathy = compute_f1(logits_empathy, label_empathy_ids, axis_=1)\n",
    "        macro_f1_rationale = compute_f1_rationale(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "        \n",
    "        iou_f1_rationale = iou_f1(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "        \n",
    "        total_pos_f1_empathy += pos_f1_empathy\n",
    "        total_micro_f1_empathy += micro_f1_empathy\n",
    "        total_macro_f1_empathy += macro_f1_empathy\n",
    "        \n",
    "        total_macro_f1_rationale += macro_f1_rationale\n",
    "        total_iou_rationale += iou_f1_rationale\n",
    "\n",
    "    avg_val_accuracy_empathy = total_eval_accuracy_empathy / len(validation_dataloader)\n",
    "    avg_val_accuracy_rationale = total_eval_accuracy_rationale / len(validation_dataloader)\n",
    "    \n",
    "    avg_val_pos_f1_empathy = total_pos_f1_empathy / len(validation_dataloader)\n",
    "    avg_val_micro_f1_empathy = total_micro_f1_empathy / len(validation_dataloader)\n",
    "    avg_val_macro_f1_empathy = total_macro_f1_empathy / len(validation_dataloader)\n",
    "    \n",
    "    avg_val_macro_f1_rationale = total_macro_f1_rationale / len(validation_dataloader)\n",
    "    avg_val_iou_rationale = total_iou_rationale / len(validation_dataloader)\n",
    "    \n",
    "    print(\"  Accuracy-Empathy: {0:.4f}\".format(avg_val_accuracy_empathy))\n",
    "    print(\"  macro_f1_empathy: {0:.4f}\".format(avg_val_macro_f1_empathy))\n",
    "    print(\"  Accuracy-Rationale: {0:.4f}\".format(avg_val_accuracy_rationale))\n",
    "    \n",
    "    print(\"  IOU-F1-Rationale: {0:.4f}\".format(avg_val_iou_rationale))\n",
    "    print(\"  macro_f1_rationale: {0:.4f}\".format(avg_val_macro_f1_rationale))\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.342222Z",
     "iopub.status.idle": "2025-05-03T11:28:42.342603Z",
     "shell.execute_reply": "2025-05-03T11:28:42.342417Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.342398Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "total_eval_accuracy_empathy = 0\n",
    "total_eval_accuracy_rationale = 0\n",
    "\n",
    "total_pos_f1_empathy = 0.0\n",
    "total_micro_f1_empathy = 0.0\n",
    "total_macro_f1_empathy = 0.0\n",
    "\n",
    "total_pos_f1_rationale = 0.0\n",
    "total_micro_f1_rationale = 0.0\n",
    "total_macro_f1_rationale = 0.0\n",
    "total_iou_rationale = 0.0\n",
    "\n",
    "total_eval_loss = 0\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    b_input_ids_SP = batch[0].to(device)\n",
    "    b_input_mask_SP = batch[1].to(device)\n",
    "    b_input_ids_RP = batch[2].to(device)\n",
    "    b_input_mask_RP = batch[3].to(device)\n",
    "    b_labels = batch[4].to(device)\n",
    "    b_rationales = batch[5].to(device)\n",
    "    b_rationales_trimmed = batch[6].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = model(input_ids_SP = b_input_ids_SP,\n",
    "                                                            input_ids_RP = b_input_ids_RP, \n",
    "                                                            attention_mask_SP=b_input_mask_SP,\n",
    "                                                            attention_mask_RP=b_input_mask_RP, \n",
    "                                                            empathy_labels=b_labels,\n",
    "                                                            rationale_labels=b_rationales,\n",
    "                                                            lambda_EI=lambda_EI,\n",
    "                                                            lambda_RE=lambda_RE)\n",
    "\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    logits_empathy = logits_empathy.detach().cpu().numpy()\n",
    "    logits_rationale = logits_rationale.detach().cpu().numpy()\n",
    "    \n",
    "    label_empathy_ids = b_labels.to('cpu').numpy()\n",
    "    label_rationale_ids = b_rationales.to('cpu').numpy()\n",
    "    rationale_lens = b_rationales_trimmed.to('cpu').numpy()\n",
    "\n",
    "    total_eval_accuracy_empathy += flat_accuracy(logits_empathy, label_empathy_ids, axis_=1)\n",
    "    total_eval_accuracy_rationale += flat_accuracy_rationale(logits_rationale, label_rationale_ids,  label_empathy_ids, rationale_lens, axis_=2)\n",
    "    \n",
    "    pos_f1_empathy, micro_f1_empathy, macro_f1_empathy = compute_f1(logits_empathy, label_empathy_ids, axis_=1)\n",
    "    macro_f1_rationale = compute_f1_rationale(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "\n",
    "    iou_f1_rationale = iou_f1(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "\n",
    "    total_pos_f1_empathy += pos_f1_empathy\n",
    "    total_micro_f1_empathy += micro_f1_empathy\n",
    "    total_macro_f1_empathy += macro_f1_empathy\n",
    "\n",
    "    total_macro_f1_rationale += macro_f1_rationale\n",
    "    total_iou_rationale += iou_f1_rationale\n",
    "\n",
    "avg_test_accuracy_empathy = total_eval_accuracy_empathy / len(test_dataloader)\n",
    "avg_test_accuracy_rationale = total_eval_accuracy_rationale / len(test_dataloader)\n",
    "\n",
    "avg_test_pos_f1_empathy = total_pos_f1_empathy / len(test_dataloader)\n",
    "avg_test_micro_f1_empathy = total_micro_f1_empathy / len(test_dataloader)\n",
    "avg_test_macro_f1_empathy = total_macro_f1_empathy / len(test_dataloader)\n",
    "\n",
    "avg_test_macro_f1_rationale = total_macro_f1_rationale / len(test_dataloader)\n",
    "avg_test_iou_rationale = total_iou_rationale / len(test_dataloader)\n",
    "\n",
    "print(\"  Accuracy-Empathy: {0:.4f}\".format(avg_test_accuracy_empathy))\n",
    "print(\"  macro_f1_empathy: {0:.4f}\".format(avg_test_macro_f1_empathy))\n",
    "print(\"  Accuracy-Rationale: {0:.4f}\".format(avg_test_accuracy_rationale))\n",
    "\n",
    "print(\"  IOU-F1-Rationale: {0:.4f}\".format(avg_test_iou_rationale))\n",
    "print(\"  macro_f1_rationale: {0:.4f}\".format(avg_test_macro_f1_rationale))\n",
    "\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.343470Z",
     "iopub.status.idle": "2025-05-03T11:28:42.343760Z",
     "shell.execute_reply": "2025-05-03T11:28:42.343609Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.343599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.344520Z",
     "iopub.status.idle": "2025-05-03T11:28:42.344762Z",
     "shell.execute_reply": "2025-05-03T11:28:42.344668Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.344659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.contrib.empathy_models.empathy_classifier import EmpathyClassifier\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.345927Z",
     "iopub.status.idle": "2025-05-03T11:28:42.346131Z",
     "shell.execute_reply": "2025-05-03T11:28:42.346042Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.346033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_path = '/kaggle/working/Empathy-Mental-Health/dataset/sample_test_input.csv'\n",
    "output_path = '/kaggle/working/sample_test_input_out.csv'\n",
    "ER_model_path = \"/kaggle/working/model_output\"\n",
    "IP_model_path = \"/kaggle/working/interpretations_model_output\"\n",
    "EX_model_path = \"/kaggle/working/explorations_model_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.347352Z",
     "iopub.status.idle": "2025-05-03T11:28:42.347632Z",
     "shell.execute_reply": "2025-05-03T11:28:42.347504Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.347492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# input_df = pd.read_csv(input_path, header=0)\n",
    "input_df = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.348675Z",
     "iopub.status.idle": "2025-05-03T11:28:42.348957Z",
     "shell.execute_reply": "2025-05-03T11:28:42.348810Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.348796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ids = input_df.id.astype(str).tolist()\n",
    "seeker_posts = input_df.seeker_post.astype(str).tolist()\n",
    "response_posts = input_df.response_post.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.350020Z",
     "iopub.status.idle": "2025-05-03T11:28:42.350244Z",
     "shell.execute_reply": "2025-05-03T11:28:42.350157Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.350148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "empathy_classifier = EmpathyClassifier(device,\n",
    "\t\t\t\t\t\tER_model_path = ER_model_path, \n",
    "\t\t\t\t\t\tIP_model_path = IP_model_path,\n",
    "\t\t\t\t\t\tEX_model_path = EX_model_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.351976Z",
     "iopub.status.idle": "2025-05-03T11:28:42.352294Z",
     "shell.execute_reply": "2025-05-03T11:28:42.352151Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.352137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_file = codecs.open(output_path, 'w', 'utf-8')\n",
    "csv_writer = csv.writer(output_file, delimiter=',', quotechar='\"')\n",
    "csv_writer.writerow(['id','seeker_post','response_post','ER_label','IP_label','EX_label', 'ER_rationale', 'IP_rationale', 'EX_rationale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.353015Z",
     "iopub.status.idle": "2025-05-03T11:28:42.353302Z",
     "shell.execute_reply": "2025-05-03T11:28:42.353168Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.353154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(seeker_posts)):\n",
    "\t(logits_empathy_ER, predictions_ER, logits_empathy_IP, predictions_IP, logits_empathy_EX, predictions_EX, logits_rationale_ER, predictions_rationale_ER, logits_rationale_IP, predictions_rationale_IP, logits_rationale_EX,predictions_rationale_EX) = empathy_classifier.predict_empathy([seeker_posts[i]], [response_posts[i]])\n",
    "\n",
    "\tcsv_writer.writerow([ids[i], seeker_posts[i], response_posts[i], predictions_ER[0], predictions_IP[0], predictions_EX[0], predictions_rationale_ER[0].tolist(), predictions_rationale_IP[0].tolist(), predictions_rationale_EX[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.354646Z",
     "iopub.status.idle": "2025-05-03T11:28:42.354900Z",
     "shell.execute_reply": "2025-05-03T11:28:42.354770Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.354761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.356491Z",
     "iopub.status.idle": "2025-05-03T11:28:42.356755Z",
     "shell.execute_reply": "2025-05-03T11:28:42.356654Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.356645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "t = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.358767Z",
     "iopub.status.idle": "2025-05-03T11:28:42.358994Z",
     "shell.execute_reply": "2025-05-03T11:28:42.358891Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.358882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12122626,
     "datasetId": 7313126,
     "sourceId": 11653318,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
