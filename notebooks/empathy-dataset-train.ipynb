{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:14:28.479421Z",
     "iopub.status.busy": "2025-05-18T16:14:28.479238Z",
     "iopub.status.idle": "2025-05-18T16:14:28.501728Z",
     "shell.execute_reply": "2025-05-18T16:14:28.501290Z",
     "shell.execute_reply.started": "2025-05-18T16:14:28.479405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:27.431474Z",
     "iopub.status.busy": "2025-05-18T16:37:27.430553Z",
     "iopub.status.idle": "2025-05-18T16:37:28.319067Z",
     "shell.execute_reply": "2025-05-18T16:37:28.318413Z",
     "shell.execute_reply.started": "2025-05-18T16:37:27.431441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n",
      "Cloning into 'empathy_dataset_transfer'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 233, done.\u001b[K\n",
      "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
      "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
      "remote: Total 233 (delta 93), reused 191 (delta 58), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (233/233), 172.22 KiB | 9.06 MiB/s, done.\n",
      "Resolving deltas: 100% (93/93), done.\n"
     ]
    }
   ],
   "source": [
    "%rm -rf /kaggle/working/empathy_dataset_transfer\n",
    "%cd /kaggle/working\n",
    "!git clone https://bedanar:<GITHUB_TOKEN>@github.com/psytechlab/empathy_dataset_transfer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:29.613679Z",
     "iopub.status.busy": "2025-05-18T16:37:29.613075Z",
     "iopub.status.idle": "2025-05-18T16:37:29.806681Z",
     "shell.execute_reply": "2025-05-18T16:37:29.805806Z",
     "shell.execute_reply.started": "2025-05-18T16:37:29.613651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/empathy_dataset_transfer\n",
      "Branch 'empathy-models' set up to track remote branch 'empathy-models' from 'origin'.\n",
      "Switched to a new branch 'empathy-models'\n",
      "/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/empathy_dataset_transfer\n",
    "!git checkout empathy-models\n",
    "%cd /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:30.275870Z",
     "iopub.status.busy": "2025-05-18T16:37:30.275604Z",
     "iopub.status.idle": "2025-05-18T16:37:30.669022Z",
     "shell.execute_reply": "2025-05-18T16:37:30.668359Z",
     "shell.execute_reply.started": "2025-05-18T16:37:30.275846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/empathy_dataset_transfer\") \n",
    "sys.path.append(\"/kaggle/working/empathy_dataset_transfer/src\") \n",
    "sys.path.append(\"/kaggle/working/empathy_dataset_transfer/src/contrib\") \n",
    "sys.path.append(\"/kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:30.951207Z",
     "iopub.status.busy": "2025-05-18T16:37:30.950686Z",
     "iopub.status.idle": "2025-05-18T16:37:30.993269Z",
     "shell.execute_reply": "2025-05-18T16:37:30.992648Z",
     "shell.execute_reply.started": "2025-05-18T16:37:30.951188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaConfig\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.contrib.empathy_models.models.models import BiEncoderAttentionWithRationaleClassification\n",
    "from src.contrib.empathy_models.evaluation_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:32.926131Z",
     "iopub.status.busy": "2025-05-18T16:37:32.925411Z",
     "iopub.status.idle": "2025-05-18T16:37:32.967892Z",
     "shell.execute_reply": "2025-05-18T16:37:32.967317Z",
     "shell.execute_reply.started": "2025-05-18T16:37:32.926095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "lambda_EI = 1\n",
    "lambda_RE = 0.5\n",
    "dropout = 0.1\n",
    "max_len = 128\n",
    "batch_size = 4\n",
    "epochs = 5\n",
    "seed_val = 12\n",
    "model_name = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "else:\n",
    "\tprint('No GPU available, using the CPU instead.')\n",
    "\tdevice = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:34.520759Z",
     "iopub.status.busy": "2025-05-18T16:37:34.520509Z",
     "iopub.status.idle": "2025-05-18T16:37:34.563364Z",
     "shell.execute_reply": "2025-05-18T16:37:34.562663Z",
     "shell.execute_reply.started": "2025-05-18T16:37:34.520743Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:34.880436Z",
     "iopub.status.busy": "2025-05-18T16:37:34.880194Z",
     "iopub.status.idle": "2025-05-18T16:37:35.064209Z",
     "shell.execute_reply": "2025-05-18T16:37:35.063415Z",
     "shell.execute_reply.started": "2025-05-18T16:37:34.880420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotional-reactions-reddit.csv  interpretations-reddit.csv\n",
      "explorations-reddit.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%ls kaggle/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:37:35.777591Z",
     "iopub.status.busy": "2025-05-18T16:37:35.776860Z",
     "iopub.status.idle": "2025-05-18T16:38:11.661010Z",
     "shell.execute_reply": "2025-05-18T16:38:11.660315Z",
     "shell.execute_reply.started": "2025-05-18T16:37:35.777565Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!python /kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models/process_data.py --input_path=/kaggle/input/emotional-reactions-reddit.csv --output_path=/kaggle/working/emotional-reactions-reddit.csv\n",
    "!python /kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models/process_data.py --input_path=/kaggle/input/explorations-reddit.csv --output_path=/kaggle/working/explorations-reddit.csv\n",
    "!python /kaggle/working/empathy_dataset_transfer/src/contrib/empathy_models/process_data.py --input_path=/kaggle/input/interpretations-reddit.csv --output_path=/kaggle/working/interpretations-reddit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:38:27.050669Z",
     "iopub.status.busy": "2025-05-18T16:38:27.050372Z",
     "iopub.status.idle": "2025-05-18T16:38:27.190038Z",
     "shell.execute_reply": "2025-05-18T16:38:27.189533Z",
     "shell.execute_reply.started": "2025-05-18T16:38:27.050643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_all = pd.read_csv('/kaggle/working/emotional-reactions-reddit.csv', delimiter=',')\n",
    "# df_all = pd.read_csv('/kaggle/working/explorations-reddit.csv', delimiter=',')\n",
    "df_all = pd.read_csv('/kaggle/working/interpretations-reddit.csv', delimiter=',')\n",
    "\n",
    "df, validation = train_test_split(df_all, train_size=0.7, random_state=42)\n",
    "df_val, df_test = train_test_split(validation, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:38:27.831966Z",
     "iopub.status.busy": "2025-05-18T16:38:27.831759Z",
     "iopub.status.idle": "2025-05-18T16:38:27.872615Z",
     "shell.execute_reply": "2025-05-18T16:38:27.871963Z",
     "shell.execute_reply.started": "2025-05-18T16:38:27.831951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_path = \"/kaggle/working/sample_input_ER_out.csv\"\n",
    "# dev_path = \"/kaggle/working/sample_input_ER_out.csv\"\n",
    "# test_path = \"/kaggle/working/sample_input_ER_out.csv\"\n",
    "save_model_path = '/kaggle/working/interpretations_model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:38:28.476367Z",
     "iopub.status.busy": "2025-05-18T16:38:28.475895Z",
     "iopub.status.idle": "2025-05-18T16:38:28.618739Z",
     "shell.execute_reply": "2025-05-18T16:38:28.618268Z",
     "shell.execute_reply.started": "2025-05-18T16:38:28.476350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(train_path, delimiter=',')\n",
    "df['rationale_labels'] = df['rationale_labels'].apply(lambda s: torch.tensor(np.asarray([int(i) for i in s.split(',')]), dtype=torch.long))\n",
    "\n",
    "# df_test = pd.read_csv(test_path, delimiter=',')\n",
    "df_test['rationale_labels'] = df_test['rationale_labels'].apply(lambda s: torch.tensor(np.asarray([int(i) for i in s.split(',')]), dtype=torch.long))\n",
    "\n",
    "# df_val = pd.read_csv(dev_path, delimiter=',')\n",
    "df_val['rationale_labels'] = df_val['rationale_labels'].apply(lambda s: torch.tensor(np.asarray([int(i) for i in s.split(',')]), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:38:54.275521Z",
     "iopub.status.busy": "2025-05-18T16:38:54.275256Z",
     "iopub.status.idle": "2025-05-18T16:38:54.317050Z",
     "shell.execute_reply": "2025-05-18T16:38:54.316354Z",
     "shell.execute_reply.started": "2025-05-18T16:38:54.275502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# set([len(i) for i in tokenizer_RP['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:40:07.738210Z",
     "iopub.status.busy": "2025-05-18T16:40:07.737611Z",
     "iopub.status.idle": "2025-05-18T16:40:08.686368Z",
     "shell.execute_reply": "2025-05-18T16:40:08.685561Z",
     "shell.execute_reply.started": "2025-05-18T16:40:07.738185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = RobertaTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer_RP = tokenizer.batch_encode_plus(df.response_post.tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "input_ids_RP = torch.tensor(tokenizer_RP['input_ids'])\n",
    "attention_masks_RP = torch.tensor(tokenizer_RP['attention_mask'])\n",
    "\n",
    "tokenizer_SP = tokenizer.batch_encode_plus(df.seeker_post.tolist(), add_special_tokens=True, max_length=max_len, truncation=True, padding='max_length', return_attention_mask=True)\n",
    "input_ids_SP = torch.tensor(tokenizer_SP['input_ids'])\n",
    "attention_masks_SP = torch.tensor(tokenizer_SP['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:40:14.675482Z",
     "iopub.status.busy": "2025-05-18T16:40:14.674865Z",
     "iopub.status.idle": "2025-05-18T16:40:14.733085Z",
     "shell.execute_reply": "2025-05-18T16:40:14.732159Z",
     "shell.execute_reply.started": "2025-05-18T16:40:14.675460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = df.level.values.astype(int)\n",
    "labels = torch.tensor(labels)\n",
    "rationales = df.rationale_labels.values.tolist()\n",
    "rationales = torch.stack(rationales, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:40:38.341071Z",
     "iopub.status.busy": "2025-05-18T16:40:38.340380Z",
     "iopub.status.idle": "2025-05-18T16:40:38.498175Z",
     "shell.execute_reply": "2025-05-18T16:40:38.497639Z",
     "shell.execute_reply.started": "2025-05-18T16:40:38.341049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_tokenizer_RP = tokenizer.batch_encode_plus(df_val.response_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length', return_attention_mask=True)\n",
    "val_input_ids_RP = torch.tensor(val_tokenizer_RP['input_ids'])\n",
    "val_attention_masks_RP = torch.tensor(val_tokenizer_RP['attention_mask'])\n",
    "\n",
    "val_tokenizer_SP = tokenizer.batch_encode_plus(df_val.seeker_post.tolist(), add_special_tokens=True, truncation=True, max_length=max_len, padding='max_length', return_attention_mask=True)\n",
    "val_input_ids_SP = torch.tensor(val_tokenizer_SP['input_ids'])\n",
    "val_attention_masks_SP = torch.tensor(val_tokenizer_SP['attention_mask'])\n",
    "\n",
    "val_labels = torch.tensor(df_val.level.values.astype(int))\n",
    "val_rationales = df_val.rationale_labels.values.tolist()\n",
    "val_rationales = torch.stack(val_rationales, dim=0)\n",
    "val_rationales_trimmed = torch.tensor(df_val.rationale_labels_trimmed.values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:02.464466Z",
     "iopub.status.busy": "2025-05-18T16:41:02.464199Z",
     "iopub.status.idle": "2025-05-18T16:41:02.616849Z",
     "shell.execute_reply": "2025-05-18T16:41:02.616147Z",
     "shell.execute_reply.started": "2025-05-18T16:41:02.464447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_tokenizer_RP = tokenizer.batch_encode_plus(df_test.response_post.tolist(), add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True, return_attention_mask=True)\n",
    "test_input_ids_RP = torch.tensor(test_tokenizer_RP['input_ids'])\n",
    "test_attention_masks_RP = torch.tensor(test_tokenizer_RP['attention_mask'])\n",
    "\n",
    "test_tokenizer_SP = tokenizer.batch_encode_plus(df_test.seeker_post.tolist(), add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True, return_attention_mask=True)\n",
    "test_input_ids_SP = torch.tensor(test_tokenizer_SP['input_ids'])\n",
    "test_attention_masks_SP = torch.tensor(test_tokenizer_SP['attention_mask'])\n",
    "\n",
    "test_labels = torch.tensor(df_test.level.values.astype(int))\n",
    "test_rationales = df_test.rationale_labels.values.tolist()\n",
    "test_rationales = torch.stack(test_rationales, dim=0)\n",
    "test_rationales_trimmed = torch.tensor(df_test.rationale_labels_trimmed.values.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:06.724218Z",
     "iopub.status.busy": "2025-05-18T16:41:06.723667Z",
     "iopub.status.idle": "2025-05-18T16:41:29.587693Z",
     "shell.execute_reply": "2025-05-18T16:41:29.587145Z",
     "shell.execute_reply.started": "2025-05-18T16:41:06.724195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 16:41:12.607279: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747586472.798965      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747586472.853894      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616cbc55faea4119acdbde0035796ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecf26c7aa4c4e89917c29deb98b3ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BiEncoderAttentionWithRationaleClassification(model_name=model_name, hidden_dropout_prob=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:29.589517Z",
     "iopub.status.busy": "2025-05-18T16:41:29.588994Z",
     "iopub.status.idle": "2025-05-18T16:41:29.661390Z",
     "shell.execute_reply": "2025-05-18T16:41:29.660788Z",
     "shell.execute_reply.started": "2025-05-18T16:41:29.589498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "for p in model.seeker_encoder.parameters():\n",
    "\tp.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:29.662678Z",
     "iopub.status.busy": "2025-05-18T16:41:29.662425Z",
     "iopub.status.idle": "2025-05-18T16:41:29.732711Z",
     "shell.execute_reply": "2025-05-18T16:41:29.732129Z",
     "shell.execute_reply.started": "2025-05-18T16:41:29.662654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = lr, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:34.292522Z",
     "iopub.status.busy": "2025-05-18T16:41:34.291979Z",
     "iopub.status.idle": "2025-05-18T16:41:34.371509Z",
     "shell.execute_reply": "2025-05-18T16:41:34.370778Z",
     "shell.execute_reply.started": "2025-05-18T16:41:34.292499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_SP, attention_masks_SP, input_ids_RP, attention_masks_RP, labels, rationales)\n",
    "train_size = int(len(train_dataset))\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(val_input_ids_SP, val_attention_masks_SP, val_input_ids_RP, val_attention_masks_RP, val_labels, val_rationales, val_rationales_trimmed)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids_SP, test_attention_masks_SP, test_input_ids_RP, test_attention_masks_RP, test_labels, test_rationales, test_rationales_trimmed)\n",
    "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:36.105346Z",
     "iopub.status.busy": "2025-05-18T16:41:36.104595Z",
     "iopub.status.idle": "2025-05-18T16:41:36.382692Z",
     "shell.execute_reply": "2025-05-18T16:41:36.381930Z",
     "shell.execute_reply.started": "2025-05-18T16:41:36.105313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total_steps = len(train_dataloader) * epochs\n",
    "num_batch = len(train_dataloader)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:41:36.655228Z",
     "iopub.status.busy": "2025-05-18T16:41:36.654979Z",
     "iopub.status.idle": "2025-05-18T16:51:19.019806Z",
     "shell.execute_reply": "2025-05-18T16:51:19.019119Z",
     "shell.execute_reply.started": "2025-05-18T16:41:36.655210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 540/540 [01:39<00:00,  5.45it/s, total loss: 0.9001 epoch: 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy-Empathy: 0.8182\n",
      "  macro_f1_empathy: 0.7415\n",
      "  Accuracy-Rationale: 0.6187\n",
      "  IOU-F1-Rationale: 0.5030\n",
      "  macro_f1_rationale: 0.5757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 540/540 [01:49<00:00,  4.91it/s, total loss: 0.7217 epoch: 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy-Empathy: 0.8125\n",
      "  macro_f1_empathy: 0.7377\n",
      "  Accuracy-Rationale: 0.6107\n",
      "  IOU-F1-Rationale: 0.5130\n",
      "  macro_f1_rationale: 0.5845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 540/540 [01:49<00:00,  4.94it/s, total loss: 0.6034 epoch: 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy-Empathy: 0.8103\n",
      "  macro_f1_empathy: 0.7245\n",
      "  Accuracy-Rationale: 0.5842\n",
      "  IOU-F1-Rationale: 0.5906\n",
      "  macro_f1_rationale: 0.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 540/540 [01:49<00:00,  4.94it/s, total loss: 0.4457 epoch: 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy-Empathy: 0.8103\n",
      "  macro_f1_empathy: 0.7320\n",
      "  Accuracy-Rationale: 0.5791\n",
      "  IOU-F1-Rationale: 0.5881\n",
      "  macro_f1_rationale: 0.6148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|██████████| 540/540 [01:49<00:00,  4.94it/s, total loss: 0.3513 epoch: 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy-Empathy: 0.8168\n",
      "  macro_f1_empathy: 0.7379\n",
      "  Accuracy-Rationale: 0.5984\n",
      "  IOU-F1-Rationale: 0.5890\n",
      "  macro_f1_rationale: 0.6089\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, epochs):\n",
    "    total_train_loss = 0\n",
    "    total_train_empathy_loss = 0\n",
    "    total_train_rationale_loss = 0\n",
    "    \n",
    "    pbar = tqdm(total=num_batch, desc=f\"training\")\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids_SP = batch[0].to(device)\n",
    "        b_input_mask_SP = batch[1].to(device)\n",
    "        b_input_ids_RP = batch[2].to(device)\n",
    "        b_input_mask_RP = batch[3].to(device)\n",
    "        b_labels = batch[4].to(device)\n",
    "        b_rationales = batch[5].to(device)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "        \n",
    "        loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = model(input_ids_SP = b_input_ids_SP,\n",
    "                                                                input_ids_RP = b_input_ids_RP, \n",
    "                                                                attention_mask_SP=b_input_mask_SP,\n",
    "                                                                attention_mask_RP=b_input_mask_RP, \n",
    "                                                                empathy_labels=b_labels,\n",
    "                                                                rationale_labels=b_rationales,\n",
    "                                                                lambda_EI=lambda_EI,\n",
    "                                                                lambda_RE=lambda_RE)\n",
    "        \n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        total_train_empathy_loss += loss_empathy.item()\n",
    "        total_train_rationale_loss += loss_rationale.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar.set_postfix_str(\n",
    "            f\"total loss: {float(total_train_loss/(step+1)):.4f} epoch: {epoch_i}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_empathy_loss = total_train_empathy_loss / len(train_dataloader)\n",
    "    avg_train_rationale_loss = total_train_rationale_loss / len(train_dataloader)\n",
    "    \n",
    "    pbar.close()\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy_empathy = 0\n",
    "    total_eval_accuracy_rationale = 0\n",
    "    \n",
    "    total_pos_f1_empathy = 0.0\n",
    "    total_micro_f1_empathy = 0.0\n",
    "    total_macro_f1_empathy = 0.0\n",
    "    \n",
    "    total_pos_f1_rationale = 0.0\n",
    "    total_micro_f1_rationale = 0.0\n",
    "    total_macro_f1_rationale = 0.0\n",
    "    \n",
    "    total_iou_rationale = 0.0\n",
    "    \n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:   \n",
    "        b_input_ids_SP = batch[0].to(device)\n",
    "        b_input_mask_SP = batch[1].to(device)\n",
    "        b_input_ids_RP = batch[2].to(device)\n",
    "        b_input_mask_RP = batch[3].to(device)\n",
    "        b_labels = batch[4].to(device)\n",
    "        b_rationales = batch[5].to(device)\t\n",
    "        b_rationales_trimmed = batch[6].to(device)\t\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = model(input_ids_SP = b_input_ids_SP,\n",
    "                                                                input_ids_RP = b_input_ids_RP, \n",
    "                                                                attention_mask_SP=b_input_mask_SP,\n",
    "                                                                attention_mask_RP=b_input_mask_RP, \n",
    "                                                                empathy_labels=b_labels,\n",
    "                                                                rationale_labels=b_rationales,\n",
    "                                                                lambda_EI=lambda_EI,\n",
    "                                                                lambda_RE=lambda_RE)\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        logits_empathy = logits_empathy.detach().cpu().numpy()\n",
    "        logits_rationale = logits_rationale.detach().cpu().numpy()\n",
    "        \n",
    "        label_empathy_ids = b_labels.to('cpu').numpy()\n",
    "        label_rationale_ids = b_rationales.to('cpu').numpy()\n",
    "        rationale_lens = b_rationales_trimmed.to('cpu').numpy()\n",
    "        \n",
    "        total_eval_accuracy_empathy += flat_accuracy(logits_empathy, label_empathy_ids, axis_=1)\n",
    "        total_eval_accuracy_rationale += flat_accuracy_rationale(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "        \n",
    "        pos_f1_empathy, micro_f1_empathy, macro_f1_empathy = compute_f1(logits_empathy, label_empathy_ids, axis_=1)\n",
    "        macro_f1_rationale = compute_f1_rationale(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "        \n",
    "        iou_f1_rationale = iou_f1(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "        \n",
    "        total_pos_f1_empathy += pos_f1_empathy\n",
    "        total_micro_f1_empathy += micro_f1_empathy\n",
    "        total_macro_f1_empathy += macro_f1_empathy\n",
    "        \n",
    "        total_macro_f1_rationale += macro_f1_rationale\n",
    "        total_iou_rationale += iou_f1_rationale\n",
    "\n",
    "    avg_val_accuracy_empathy = total_eval_accuracy_empathy / len(validation_dataloader)\n",
    "    avg_val_accuracy_rationale = total_eval_accuracy_rationale / len(validation_dataloader)\n",
    "    \n",
    "    avg_val_pos_f1_empathy = total_pos_f1_empathy / len(validation_dataloader)\n",
    "    avg_val_micro_f1_empathy = total_micro_f1_empathy / len(validation_dataloader)\n",
    "    avg_val_macro_f1_empathy = total_macro_f1_empathy / len(validation_dataloader)\n",
    "    \n",
    "    avg_val_macro_f1_rationale = total_macro_f1_rationale / len(validation_dataloader)\n",
    "    avg_val_iou_rationale = total_iou_rationale / len(validation_dataloader)\n",
    "    \n",
    "    print(\"  Accuracy-Empathy: {0:.4f}\".format(avg_val_accuracy_empathy))\n",
    "    print(\"  macro_f1_empathy: {0:.4f}\".format(avg_val_macro_f1_empathy))\n",
    "    print(\"  Accuracy-Rationale: {0:.4f}\".format(avg_val_accuracy_rationale))\n",
    "    \n",
    "    print(\"  IOU-F1-Rationale: {0:.4f}\".format(avg_val_iou_rationale))\n",
    "    print(\"  macro_f1_rationale: {0:.4f}\".format(avg_val_macro_f1_rationale))\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:51:19.021402Z",
     "iopub.status.busy": "2025-05-18T16:51:19.021075Z",
     "iopub.status.idle": "2025-05-18T16:51:28.093391Z",
     "shell.execute_reply": "2025-05-18T16:51:28.092644Z",
     "shell.execute_reply.started": "2025-05-18T16:51:19.021384Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy-Empathy: 0.8362\n",
      "  macro_f1_empathy: 0.7841\n",
      "  Accuracy-Rationale: 0.6084\n",
      "  IOU-F1-Rationale: 0.6199\n",
      "  macro_f1_rationale: 0.6180\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "total_eval_accuracy_empathy = 0\n",
    "total_eval_accuracy_rationale = 0\n",
    "\n",
    "total_pos_f1_empathy = 0.0\n",
    "total_micro_f1_empathy = 0.0\n",
    "total_macro_f1_empathy = 0.0\n",
    "\n",
    "total_pos_f1_rationale = 0.0\n",
    "total_micro_f1_rationale = 0.0\n",
    "total_macro_f1_rationale = 0.0\n",
    "total_iou_rationale = 0.0\n",
    "\n",
    "total_eval_loss = 0\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    b_input_ids_SP = batch[0].to(device)\n",
    "    b_input_mask_SP = batch[1].to(device)\n",
    "    b_input_ids_RP = batch[2].to(device)\n",
    "    b_input_mask_RP = batch[3].to(device)\n",
    "    b_labels = batch[4].to(device)\n",
    "    b_rationales = batch[5].to(device)\n",
    "    b_rationales_trimmed = batch[6].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss, loss_empathy, loss_rationale, logits_empathy, logits_rationale = model(input_ids_SP = b_input_ids_SP,\n",
    "                                                            input_ids_RP = b_input_ids_RP, \n",
    "                                                            attention_mask_SP=b_input_mask_SP,\n",
    "                                                            attention_mask_RP=b_input_mask_RP, \n",
    "                                                            empathy_labels=b_labels,\n",
    "                                                            rationale_labels=b_rationales,\n",
    "                                                            lambda_EI=lambda_EI,\n",
    "                                                            lambda_RE=lambda_RE)\n",
    "\n",
    "    total_eval_loss += loss.item()\n",
    "\n",
    "    logits_empathy = logits_empathy.detach().cpu().numpy()\n",
    "    logits_rationale = logits_rationale.detach().cpu().numpy()\n",
    "    \n",
    "    label_empathy_ids = b_labels.to('cpu').numpy()\n",
    "    label_rationale_ids = b_rationales.to('cpu').numpy()\n",
    "    rationale_lens = b_rationales_trimmed.to('cpu').numpy()\n",
    "\n",
    "    total_eval_accuracy_empathy += flat_accuracy(logits_empathy, label_empathy_ids, axis_=1)\n",
    "    total_eval_accuracy_rationale += flat_accuracy_rationale(logits_rationale, label_rationale_ids,  label_empathy_ids, rationale_lens, axis_=2)\n",
    "    \n",
    "    pos_f1_empathy, micro_f1_empathy, macro_f1_empathy = compute_f1(logits_empathy, label_empathy_ids, axis_=1)\n",
    "    macro_f1_rationale = compute_f1_rationale(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "\n",
    "    iou_f1_rationale = iou_f1(logits_rationale, label_rationale_ids, label_empathy_ids, rationale_lens, axis_=2)\n",
    "\n",
    "    total_pos_f1_empathy += pos_f1_empathy\n",
    "    total_micro_f1_empathy += micro_f1_empathy\n",
    "    total_macro_f1_empathy += macro_f1_empathy\n",
    "\n",
    "    total_macro_f1_rationale += macro_f1_rationale\n",
    "    total_iou_rationale += iou_f1_rationale\n",
    "\n",
    "avg_test_accuracy_empathy = total_eval_accuracy_empathy / len(test_dataloader)\n",
    "avg_test_accuracy_rationale = total_eval_accuracy_rationale / len(test_dataloader)\n",
    "\n",
    "avg_test_pos_f1_empathy = total_pos_f1_empathy / len(test_dataloader)\n",
    "avg_test_micro_f1_empathy = total_micro_f1_empathy / len(test_dataloader)\n",
    "avg_test_macro_f1_empathy = total_macro_f1_empathy / len(test_dataloader)\n",
    "\n",
    "avg_test_macro_f1_rationale = total_macro_f1_rationale / len(test_dataloader)\n",
    "avg_test_iou_rationale = total_iou_rationale / len(test_dataloader)\n",
    "\n",
    "print(\"  Accuracy-Empathy: {0:.4f}\".format(avg_test_accuracy_empathy))\n",
    "print(\"  macro_f1_empathy: {0:.4f}\".format(avg_test_macro_f1_empathy))\n",
    "print(\"  Accuracy-Rationale: {0:.4f}\".format(avg_test_accuracy_rationale))\n",
    "\n",
    "print(\"  IOU-F1-Rationale: {0:.4f}\".format(avg_test_iou_rationale))\n",
    "print(\"  macro_f1_rationale: {0:.4f}\".format(avg_test_macro_f1_rationale))\n",
    "\n",
    "avg_test_loss = total_eval_loss / len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T16:51:37.508355Z",
     "iopub.status.busy": "2025-05-18T16:51:37.508063Z",
     "iopub.status.idle": "2025-05-18T16:51:39.894126Z",
     "shell.execute_reply": "2025-05-18T16:51:39.893347Z",
     "shell.execute_reply.started": "2025-05-18T16:51:37.508335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.344520Z",
     "iopub.status.idle": "2025-05-03T11:28:42.344762Z",
     "shell.execute_reply": "2025-05-03T11:28:42.344668Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.344659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from src.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.345927Z",
     "iopub.status.idle": "2025-05-03T11:28:42.346131Z",
     "shell.execute_reply": "2025-05-03T11:28:42.346042Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.346033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_path = '/kaggle/working/Empathy-Mental-Health/dataset/sample_test_input.csv'\n",
    "output_path = '/kaggle/working/sample_test_input_out.csv'\n",
    "ER_model_path = \"/kaggle/working/model_output\"\n",
    "IP_model_path = \"/kaggle/working/interpretations_model_output\"\n",
    "EX_model_path = \"/kaggle/working/explorations_model_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.347352Z",
     "iopub.status.idle": "2025-05-03T11:28:42.347632Z",
     "shell.execute_reply": "2025-05-03T11:28:42.347504Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.347492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# input_df = pd.read_csv(input_path, header=0)\n",
    "input_df = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.348675Z",
     "iopub.status.idle": "2025-05-03T11:28:42.348957Z",
     "shell.execute_reply": "2025-05-03T11:28:42.348810Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.348796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ids = input_df.id.astype(str).tolist()\n",
    "seeker_posts = input_df.seeker_post.astype(str).tolist()\n",
    "response_posts = input_df.response_post.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.350020Z",
     "iopub.status.idle": "2025-05-03T11:28:42.350244Z",
     "shell.execute_reply": "2025-05-03T11:28:42.350157Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.350148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "empathy_classifier = EmpathyClassifier(device,\n",
    "\t\t\t\t\t\tER_model_path = ER_model_path, \n",
    "\t\t\t\t\t\tIP_model_path = IP_model_path,\n",
    "\t\t\t\t\t\tEX_model_path = EX_model_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.351976Z",
     "iopub.status.idle": "2025-05-03T11:28:42.352294Z",
     "shell.execute_reply": "2025-05-03T11:28:42.352151Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.352137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_file = codecs.open(output_path, 'w', 'utf-8')\n",
    "csv_writer = csv.writer(output_file, delimiter=',', quotechar='\"')\n",
    "csv_writer.writerow(['id','seeker_post','response_post','ER_label','IP_label','EX_label', 'ER_rationale', 'IP_rationale', 'EX_rationale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.353015Z",
     "iopub.status.idle": "2025-05-03T11:28:42.353302Z",
     "shell.execute_reply": "2025-05-03T11:28:42.353168Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.353154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(seeker_posts)):\n",
    "\t(logits_empathy_ER, predictions_ER, logits_empathy_IP, predictions_IP, logits_empathy_EX, predictions_EX, logits_rationale_ER, predictions_rationale_ER, logits_rationale_IP, predictions_rationale_IP, logits_rationale_EX,predictions_rationale_EX) = empathy_classifier.predict_empathy([seeker_posts[i]], [response_posts[i]])\n",
    "\n",
    "\tcsv_writer.writerow([ids[i], seeker_posts[i], response_posts[i], predictions_ER[0], predictions_IP[0], predictions_EX[0], predictions_rationale_ER[0].tolist(), predictions_rationale_IP[0].tolist(), predictions_rationale_EX[0].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.354646Z",
     "iopub.status.idle": "2025-05-03T11:28:42.354900Z",
     "shell.execute_reply": "2025-05-03T11:28:42.354770Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.354761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.356491Z",
     "iopub.status.idle": "2025-05-03T11:28:42.356755Z",
     "shell.execute_reply": "2025-05-03T11:28:42.356654Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.356645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "t = pd.read_csv(\"/kaggle/working/sample_test_input_out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.357504Z",
     "iopub.status.idle": "2025-05-03T11:28:42.357786Z",
     "shell.execute_reply": "2025-05-03T11:28:42.357658Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.357646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_df.loc[2291, 'response_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-03T11:28:42.358767Z",
     "iopub.status.idle": "2025-05-03T11:28:42.358994Z",
     "shell.execute_reply": "2025-05-03T11:28:42.358891Z",
     "shell.execute_reply.started": "2025-05-03T11:28:42.358882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7313126,
     "sourceId": 11653318,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
