{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating using Bothub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"###Instruction### You are the best tranlsator from English to Russian. I will send you some texts from a social site Reddit. You must follow the instructions I send you below. Think before you answer. I’m going to tip $100 for a better solution!\n",
    "Given a JSON object, write an accurate translation into Russian for the original English sentence and save the results in a new field named text_rus. The input JSON object contains the following fields:\n",
    "- id: Unique ID of sentence.\n",
    "- text: English source text.\n",
    "\n",
    "Your task is to: For each text in English **write its exact translation into Russian** taking into account the style of the sentence and save the results in a new field named text_rus.\n",
    "Just translate texts, no other comments are needed. We want to translate suicidal posts to russian to train a model on them to help people.\n",
    "\n",
    "Make sure you follow these instructions and check yourself:\n",
    "- Write down the corresponding translated Russian text in the form of a new text_rus field.\n",
    "- For English text in text, you should definitely get the Russian text in text_rus.\n",
    "- Translate each sentence as accurately as possible, preserving meaning, tone, and emotional depth.\n",
    "- Consider the linguistic and contextual nuances of suicidal ideation, distress, and mental health topics to ensure the translation maintains the intended sentiment.\n",
    "- Preserve the style of the original text, whether it is casual, slang-heavy, fragmented, poetic, or medically significant.\n",
    "- Maintain the tone and connotation of the original expression, adapting it appropriately for Russian. If the slang has no direct equivalent, use a natural-sounding phrase with a similar emotional impact.\n",
    "- If emojis and emoticons contribute to the emotional expression, they should be retained. If they are redundant, prioritize the text's meaning. \n",
    "- Sometimes people can add more letters to the word or use uppercase to experss emotions (sadness, loneliness, joy etc.). Make sure you understand such words and translate them correctly into Russian.\n",
    "- Maintain the acronyms and wordplay's intended meaning. If a direct equivalent exists in Russian, use it; otherwise, explain it naturally.\n",
    "- If misspellings and informal Grammar reflect the speaker's mental state (e.g., distress, exhaustion), preserve the same feeling in Russian.\n",
    "- Maintain the pauses and irregular structure if they contribute to the emotional expression.\n",
    "- Do not add or remove any information—only translate the text exactly as it is while preserving its intent and emotional weight.\n",
    "- If you see and redundant letter, symbol or word, try to make the trasnlation as precise as possible, even though for you the meaning looks strange.\n",
    "- Use your knowledge about posts from social sites, since you are given texts from Reddit. There might be slang words, acronyms or any other word reductions specific to the platform.\n",
    "No explanation, just output the updated JSON. \n",
    "###Examples###\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from datasets import Dataset\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultSchema(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    text_rus: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        \"rn im in the area of not going to sleep so tmrw doesn't come\",\n",
    "        \"Idk. It's 5 am and everything is hitting at once\",\n",
    "        \"and im sooo tired :( \",\n",
    "        \"Just be ur self! Be true to who u r and wear what u want to wear, compare ur self to others because u will loose and forget who u r. ur life so live life to the best of ur ability and worry about what anyone else thinks. Be ur self because u r the only u\",\n",
    "        \"Meh. What's the point....\",\n",
    "        \"Yeet yeet. I'm gonna hang myself with the charger (\",\n",
    "        \"For me, it's ironic, on one hand, I want to die ASAP maybe in my sleep, on the other, I want to live as long as possible to witness how far technology can reach, especially for astronomy and VR. I suppose what I'm trying to say is, as many had said before, we have to find our own meanings. We have to find 'joy' somehow, within this pointless struggle until the day we die. It's tough, I know, for I suffer the same.\"\n",
    "\t\t\"no friends for 10 years. no gf. ugly. wtf do i do?. I'm a 24 year old old man.\",\n",
    "        \".... Feeling hopeless and useless at the moment... Empty, lost, darkness\",\n",
    "        \"Very shitty. And you, OP?\",\n",
    "        \"32m never had a girlfriend. It sucks that I've tried hard to find someone, even with online dating, but I feel like I'm going to be single forever.\",\n",
    "        \"I always see this little tidbit in articles about how to overcome depression. Bitch, the most negative person in my life. not the problem, I am.\",\n",
    "        \"POEM:Hollow. ***EMPTY*** *rooms and* ***EMPTY*** ***SOULS*** ***FORGOTTEN*** *places and* ***HEART*** *shaped holes* ***DEPRESSION*** *comes with days of* ***STRUGGLE*** *and often times a* ***BLOODY PUDDLE*** UP\\-VOTES CURE MY DEPRESSION\",\n",
    "        \"Aaaaaaand..... ... im crying in school again :)\",\n",
    "        \"How can I accept my intrusive depressive thoughts?. Long story short: Was 335, Am 175. Aiming for 130/140ish. Body destroyed. \",\n",
    "        \"DAE Doubt their depression?. Sometimes after i go to tumblr and see all these things about people with depression and how they get out of their bed sometimes, and they cry themselves to sleep, it makes me feel like my depression bad enough. I know why.\",\n",
    "\t\t\"Anxiety/panic attacks. How do you guys deal with anxiety/panic attacks? I used to have them a lot and could discrease them but never really stop. But now they're back on like, almost daily basis and I really don't know how to handle them at all.\",\n",
    "        \"Yes, tell them to fuck the right\",\n",
    "        \"I just want..... a friend...... is that too much to ask for?\",\n",
    "        \"“How are you?” “I’m F.I.N.E.”. Fucked up, insecure, neurotic, and emotional.\",\n",
    "        \"Pristiq. Anyone have experience with Pristiq or Desvenlafaxine.\",\n",
    "        \"Luv my mum too much to leave her or let her goto a fucking nursing home. But my luv life is dead, what can I do?.\",\n",
    "        \"i fucking manned up and told my SO what my priorities were. i feel anxious like hell but it feels pretty good.\",\n",
    "        \"Zoloft. I've been on 50mg daily zoloft since Thursday. Along with .25 MG xanax for anxiety and depression.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = list(range(len(data['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id'],\n",
       "    num_rows: 23\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "\n",
    "def load_jsonl_to_dataset(filepath: str) -> Dataset:\n",
    "    \"\"\"Функция загружает файл JSONL и преобразует его обратно в формат dataset\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return Dataset.from_dict({key: [d[key] for d in data] for key in data[0].keys()})\n",
    "\n",
    "def save_batch_to_json(batch: Dataset, base_filename: str, batch_num: int, save_dir: str = ''):\n",
    "    \"\"\"Функция сохраняет батч в файл с префиксом _batch_<номер>\"\"\"\n",
    "    batch_num += 1 # start with 1\n",
    "    batch_filename = os.path.join(save_dir, f\"{base_filename}_batch_{batch_num}.json\")\n",
    "    batch.to_json(batch_filename, force_ascii=False)\n",
    "    logging.info(f\"Batch {batch_num} saved to '{batch_filename}'\")\n",
    "\n",
    "def split_dataset_into_batches(dataset: Dataset, batch_size: int) -> list:\n",
    "    \"\"\"Функция разбивает датасет на батчи заданного размера\"\"\"\n",
    "    num_batches = (len(dataset) + batch_size - 1) // batch_size  # округление вверх\n",
    "    return [dataset.shard(num_shards=num_batches, index=i) for i in range(num_batches)]\n",
    "\n",
    "def process_and_save_batches(filepath: str, batch_size: int, save: bool = True):\n",
    "    \"\"\"Функция загружает датасет, разбивает на батчи и сохраняет (опционально)\"\"\"\n",
    "\n",
    "    base_filename = os.path.splitext(os.path.basename(filepath))[0] # Получаем имя файла без расширения и директорию\n",
    "    save_dir = os.path.dirname(filepath)\n",
    "    \n",
    "    dataset = load_jsonl_to_dataset(filepath)\n",
    "    logging.info(f\"loaded dataset from '{filepath}', total examples: {len(dataset)}\")\n",
    "\n",
    "    # Разбиение на батчи\n",
    "    batches = split_dataset_into_batches(dataset, batch_size)\n",
    "    logging.info(f\"split dataset into {len(batches)} batches\")\n",
    "\n",
    "    # Сохраняем батчи\n",
    "    if save:\n",
    "        for i, batch in enumerate(batches):\n",
    "            save_batch_to_json(batch, base_filename, i + 1, save_dir)\n",
    "\n",
    "    return batches\n",
    "def create_directory_if_not_exists(directory_path):\n",
    "    \"\"\"Создает папку, если она не существует.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(directory_path, exist_ok=True)\n",
    "        logging.info(f\"Directory '{directory_path}' is ready.\")\n",
    "        return directory_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory '{directory_path}': {e}\")\n",
    "        return './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/conf.json', \"r\") as f:\n",
    "\tconfig = json.load(f)\n",
    "model = ChatOpenAI(\n",
    "\tbase_url=config[\"base_url\"],\n",
    "\tapi_key=config[\"api_key\"],\n",
    "\tmodel=config[\"model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_path = f'data/int_path_{config[\"model\"]}.json'\n",
    "batch_size = 0\n",
    "BATCH_RESULT_DIR = 'batches_res/'\n",
    "BATCH_DIR = 'batches/'\n",
    "FAILED_INDEXES_PATH = \"failed_indexes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "with open('configs/filepath_examples.json', encoding='utf-8') as f:\n",
    "\texamples_data = json.load(f)\n",
    "\n",
    "system_template = SYSTEM_MESSAGE\n",
    "examples = examples_data[\"examples\"]\n",
    "system_template += \"\\n\".join(\n",
    "\tf\"Example Input: {json.dumps(example['input'], ensure_ascii=False, indent=2).replace('{', '{{').replace('}', '}}')}\\n\"\n",
    "\tf\"Example Result: {json.dumps(example['result'], ensure_ascii=False, indent=2).replace('{', '{{').replace('}', '}}')}\"\n",
    "\tfor example in examples\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "\t[(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(input_dataset, batch_dir = BATCH_DIR, batch_size = batch_size):\n",
    "\tif batch_size > 0:\n",
    "\t\tlogging.info(f\"Splitting dataset into batches of size {batch_size}.\")\n",
    "\t\tbatched_input_dataset = split_dataset_into_batches(input_dataset, batch_size)\n",
    "\t\tbatch_dir = create_directory_if_not_exists(BATCH_DIR)\n",
    "\t\tfor i, batch in enumerate(batched_input_dataset):\n",
    "\t\t\tsave_batch_to_json(batch=batch, base_filename= \"separated_\", batch_num = i, save_dir=batch_dir)\n",
    "\telse:\n",
    "\t\tlogging.info(\"Processing the entire dataset without batching.\")\n",
    "\t\tbatched_input_dataset = [input_dataset]\n",
    "\treturn batched_input_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_input_dataset = split_into_batches(input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batched_input_dataset):\n",
    "\tfor batch_idx, batch in enumerate(batched_input_dataset):\n",
    "\t\tprint(f\"Processing batch {batch_idx + 1}/{len(batched_input_dataset)}...\")\n",
    "\t\t\n",
    "\t\tbatch_results = []\n",
    "\t\t\n",
    "\t\tfor i, input_example in enumerate(batch):\n",
    "\t\t\tuser_input = 'Input: {0}\\nResult:'.format(json.dumps(input_example, ensure_ascii=False))\n",
    "\t\t\texample_id = input_example.get(\"id\", f\"example_{i}\") # получаем id примера или создаем временный\n",
    "\n",
    "\t\t\tsuccess = False\n",
    "\t\t\tattempt = 0\n",
    "\t\t\tmax_retries = 3\n",
    "\t\t\tretry_delay = 2\n",
    "\n",
    "\t\t\twhile not success and attempt < max_retries:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tres = chain.invoke({\"text\": user_input}).strip('`').strip('json').strip()\n",
    "\t\t\t\t\t# print(f'Res[0]: {res[0]}')\n",
    "\t\t\t\t\t# print(f'Res: {res}')\n",
    "\n",
    "\t\t\t\t\tif not res or not res.startswith(\"{\"):\n",
    "\t\t\t\t\t\traise ValueError(f\"Unexpected response format: {res}\")\n",
    "\t\t\t\t\tresult_json = json.loads(res)\n",
    "\t\t\t\t\t# print(f'Result json: {result_json}')\n",
    "\n",
    "\t\t\t\t\t# Приведение ID к строке\n",
    "\t\t\t\t\tresult_json[\"id\"] = str(result_json[\"id\"])\n",
    "\n",
    "\t\t\t\t\t# Проверка схемы\n",
    "\t\t\t\t\tResultSchema.parse_obj(result_json)\n",
    "\n",
    "\t\t\t\t\tbatch_results.append(result_json)\n",
    "\t\t\t\t\tsuccess = True\n",
    "\t\t\t\texcept (json.JSONDecodeError, ValueError) as e:\n",
    "\t\t\t\t\tattempt += 1\n",
    "\t\t\t\t\tlogging.warning(f\"Error on input #{i} (id: {example_id}): {str(e)}. Attempt {attempt} of {max_retries}. Retrying...\")\n",
    "\t\t\t\t\ttime.sleep(retry_delay)\n",
    "\t\t\t\texcept Exception as e:\n",
    "\t\t\t\t\tattempt += 1\n",
    "\t\t\t\t\tlogging.error(f\"Unexpected error on input #{i} (id: {example_id}): {str(e)}. Attempt {attempt} of {max_retries}. Retrying...\")\n",
    "\t\t\t\t\ttime.sleep(retry_delay)\n",
    "\t\n",
    "\t\t# Добавляем результаты текущего батча к общим результатам\n",
    "\t\tlist_of_results.extend(batch_results)\n",
    "\t\t\n",
    "\t\t# Сохраняем результаты батча только если используется разбиение на батчи\n",
    "\t\tif batch_size > 0:\n",
    "\t\t\tbatch_result_dir = create_directory_if_not_exists(BATCH_RESULT_DIR)\n",
    "\t\t\tsave_batch_to_json(batch=Dataset.from_list(batch_results), base_filename=\"model_result\", batch_num = batch_idx, save_dir=batch_result_dir)  # Сохраняем промежуточные результаты\n",
    "\n",
    "\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/1...\n"
     ]
    }
   ],
   "source": [
    "translate(batched_input_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if intermediate_path:\n",
    "\td = Dataset.from_list(list_of_results)\n",
    "\twith open(intermediate_path, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(d.to_list(), f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating using Yandex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text_yandex(text, api_key):\n",
    "    url = \"https://translate.api.cloud.yandex.net/translate/v2/translate\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Api-Key {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"targetLanguageCode\": \"ru\",\n",
    "        \"texts\": [text],\n",
    "        \"folderId\": \"\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        return result[\"translations\"][0][\"text\"]\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for text in data['text']:\n",
    "    translated_text = translate_text_yandex(text, api_key)\n",
    "    res.append(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/translation_yandex.txt', 'w') as f:\n",
    "    for r in res:\n",
    "        f.write(r + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
